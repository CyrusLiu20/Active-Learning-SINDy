for i in range(experiment_repeat):
    print("Experiment Index : ", i)
    
    print("Random Exploration")
    # Random Exploration
    kernel_motivation, true_instance, est_instance = reset_explore()
    explore_policy_rand = RandomExploration(N_explore,N_horizon,power_max,objective="random")
    est_instance,cov,U_history,X_history,Frequencies_history,error_history = explore_policy_rand.system_id(true_instance,est_instance,epochs,closed_form=False,verbose=1)
    
    X_history_all.append(X_history)
    U_history_all.append(U_history)
    error_history_all.append(error_history)
    
    print("Trace Exploration")
    # Trace Exploration
    kernel_motivation, true_instance, est_instance = reset_explore()
    explore_policy_rand = TraceExploration(N_explore,N_horizon,power_max,unif_exp_frac=0)
    est_instance,cov,U_history,X_history,error_history = explore_policy_rand.system_id(true_instance,est_instance,epochs,frequencies_control=Frequencies_history,closed_form=False,verbose=1)
    
    X_history_all.append(X_history)
    U_history_all.append(U_history)
    error_history_all.append(error_history)

# with concurrent.futures.ProcessPoolExecutor() as executor:
#     futures = {executor.submit(run_experiment, i): i for i in range(experiment_repeat)}

#     # Collect results as they become available
#     for future in concurrent.futures.as_completed(futures):
#         experiment_index = futures[future]
#         try:
#             result = future.result()
#             X_history_all[experiment_index], U_history_all[experiment_index], error_history_all[experiment_index] = result
#         except Exception as e:
#             print(f"Experiment {experiment_index} failed with error: {e}")    






# Old Exploration algorithm (Exploration.py)

import torch
import numpy as np
import matplotlib.pyplot as plt
from scipy.integrate import solve_ivp
from functools import partial

lambda1_sys = 1
d_sys = 0.1
beta_sys = 1
a_sys = 0.2
p1_sys = 1
p2_sys = 1
c1_sys = 0.03
c2_sys = 0.06
b1_sys = 0.1
b2_sys = 0.01
q_sys = 0.5
h_sys = 0.1 
eta_sys = 0.9799 

class RandomExploration:
    
    def __init__(self,N,H,power_max,objective="random"):

        self.N = N
        self.H = H
        self.objective = objective
        self.power_max = power_max
    
    def system_id(self,true_instance,est_instance,epochs,closed_form=True,verbose=1):
        x = est_instance.get_states_init()
        U_history, X_history, Frequencies_history, error_history = [], [], [], []
        cov = None
        
        for epoch_idx, i in enumerate(range(len(epochs))):     
            x_history, u_history, frequencies_history,  input_power, est_hessian, cov = self.explore(true_instance,est_instance,epochs[epoch_idx],epoch_idx,past_cov=cov)
            
            U_history = U_history + u_history
            X_history = X_history + x_history
            Frequencies_history = Frequencies_history + frequencies_history
            
           
            est_instance.update_estimates(X_history,U_history,closed_form=closed_form)
            
            error_history.append(true_instance.compute_est_error(est_instance.get_dynamics()))
            model = est_instance.get_dynamics().numpy()
            if verbose == 0 or verbose == 1:
                print("Epoch ",str(epoch_idx)," :  loss :",error_history[epoch_idx])
            if verbose == 0:
                print("Model Dynamics :\n",np.round(model,3))   
        
        return est_instance, cov, U_history, X_history, Frequencies_history,  error_history

    def explore(self,true_instance,est_instance,epoch_len,epoch_idx,frequencies_control=None,past_cov=None):
        dimx, dimu, dimz = est_instance.get_dim()
        input_power = 0
        x = est_instance.get_states_init()
        dt = est_instance.get_time_step()
        n_data = int(self.H/dt)
        u_history, x_history = [],[]
        
        if past_cov is None:
            cov = torch.zeros((dimz,dimz))
        else:
            cov = past_cov
            
        # True dynamics (accelerated)
        def dynamics(t,x,u_func):
            u = u_func(t)
            x_dot = np.zeros((5, 1))
            x_dot[0] = lambda1_sys - d_sys*x[0] - beta_sys*(1 - eta_sys*u)*x[0]*x[1]
            x_dot[1] = beta_sys*(1 - eta_sys*u)*x[1]*x[0] - a_sys*x[1] - p1_sys*x[3]*x[1] - p2_sys*x[4]*x[1]
            x_dot[2] = c2_sys*x[0]*x[1]*x[2] - c2_sys*q_sys*x[1]*x[2] - b2_sys*x[2]
            x_dot[3] = c1_sys*x[1]*x[3] - b1_sys*x[3]
            x_dot[4] = c2_sys*q_sys*x[1]*x[2] - h_sys*x[4]
            return x_dot
            
        frequencies_history = []
        for i, p in enumerate(range(epoch_len)):

            if frequencies_control is None:
                frequency = np.random.uniform(0.5, 15)
            else:
                frequency = frequencies_control[i]
                
            def u_func(t):
                return np.column_stack([np.sin(frequency*t)])
            u_func_partial = partial(u_func)
            
            t_explore = torch.arange(0,self.H,dt)
            t_span = (t_explore[0], t_explore[-1])
            u_hist = torch.sin(frequency*t_explore).reshape(n_data,dimu)
            
            x_hist = torch.tensor(solve_ivp(dynamics, t_span, est_instance.get_states_init(),
                                   t_eval=t_explore,**est_instance.get_int_key(),args=(u_func_partial,)).y.T)

            frequencies_history.append(frequency)
            u_history.append(u_hist)
            x_history.append(x_hist) # Closed form estimate
        
        return x_history, u_history, frequencies_history, input_power, 0, cov
    

class TraceExploration:
    
    def __init__(self,N,H,power_max,sim_frac=0.1,ExpObj="A_Optimal"):

        self.N = N
        self.H = H
        self.power_max = power_max
        self.sim_frac = sim_frac
        self.ExpObj = ExpObj
      
    def system_id(self,true_instance,est_instance,epochs,closed_form,frequencies_control,verbose=1):
        self.closed_form = closed_form
        x = est_instance.get_states_init()
        self.U_history, self.X_history, self.Frequencies_history, self.error_history = [], [], [], []
        cov = None
        
        for epoch_idx, i in enumerate(range(len(epochs))):     
            if epoch_idx == 0:
                x_history, u_history, frequencies_history, input_power, _, cov = self.explore(true_instance,est_instance,epochs[epoch_idx],epoch_idx,frequencies_control=frequencies_control,past_cov=cov)
            else:
                x_history, u_history, frequencies_history, input_power, _, cov = self.explore(true_instance,est_instance,epochs[epoch_idx],epoch_idx,cov)
            
            self.U_history = self.U_history + u_history
            self.X_history = self.X_history + x_history
            self.Frequencies_history = self.Frequencies_history + frequencies_history
            est_instance.update_estimates(self.X_history,self.U_history,closed_form=closed_form)
            
            self.error_history.append(true_instance.compute_est_error(est_instance.get_dynamics()))
            model = est_instance.get_dynamics().numpy()
            if verbose == 0 or verbose == 1:
                print("Epoch ",str(epoch_idx)," :  loss :",self.error_history[epoch_idx])
            if verbose == 0:
                print("Model Dynamics :\n",np.round(model,3))  
        
        return est_instance, cov, self.U_history, self.X_history, self.Frequencies_history, self.error_history
    
    def explore(self,true_instance,est_instance,epoch_len,epoch_idx,frequencies_control=None,past_cov=None):
        dimx, dimu, dimz = est_instance.get_dim()
        input_power = 0
        x = est_instance.get_states_init()
        dt = est_instance.get_time_step()
        n_data = int(self.H/dt)
        u_history, x_history = [],[]
        
        if past_cov is None:
            cov = torch.zeros((dimz,dimz))
        else:
            cov = past_cov
            
        # True dynamics (accelerated)
        def dynamics(t,x,u_func):
            u = u_func(t)
            x_dot = np.zeros((5, 1))
            x_dot[0] = lambda1_sys - d_sys*x[0] - beta_sys*(1 - eta_sys*u)*x[0]*x[1]
            x_dot[1] = beta_sys*(1 - eta_sys*u)*x[1]*x[0] - a_sys*x[1] - p1_sys*x[3]*x[1] - p2_sys*x[4]*x[1]
            x_dot[2] = c2_sys*x[0]*x[1]*x[2] - c2_sys*q_sys*x[1]*x[2] - b2_sys*x[2]
            x_dot[3] = c1_sys*x[1]*x[3] - b1_sys*x[3]
            x_dot[4] = c2_sys*q_sys*x[1]*x[2] - h_sys*x[4]
            return x_dot
        
        frequencies_history = []
        if epoch_idx == 0:
            uniform = RandomExploration(self.N,self.H,self.power_max,objective="random")
            return uniform.explore(true_instance,est_instance,epoch_len,epoch_idx,frequencies_control=frequencies_control)
        else:    
            for p, i in enumerate(range(epoch_len)):
                
                frequency = self.frequency_find(est_instance,n_data)
                
                def u_func(t):
                    return np.column_stack([np.sin(frequency*t)])
                u_func_partial = partial(u_func)

                t_explore = torch.arange(0,self.H,dt)
                t_span = (t_explore[0], t_explore[-1])
                u_hist = torch.sin(frequency*t_explore).reshape(n_data,dimu)

                x_hist = torch.tensor(solve_ivp(dynamics, t_span, est_instance.get_states_init(),
                                       t_eval=t_explore,**est_instance.get_int_key(),args=(u_func_partial,)).y.T)

                u_history.append(u_hist)
                x_history.append(x_hist)
                frequencies_history.append(frequency)

            return x_history, u_history, frequencies_history, input_power, 0, cov
    
    def frequency_find(self,est_instance,n_data):
        
        frequencies = np.random.uniform(0.5, 15,size=self.N)
        costs = torch.zeros_like(torch.tensor(frequencies))
        
        for i, frequency in enumerate(frequencies):
            def u_func(t):
                return np.column_stack([np.sin(frequency*t)])
            u_func_partial = partial(u_func)

            
            t_explore = torch.arange(0,self.H*self.sim_frac,est_instance.dt)
            t_span = (t_explore[0], t_explore[-1])
            u_hist = torch.sin(frequency*t_explore).reshape(len(t_explore),est_instance.dimu)

            x_hist = torch.tensor(solve_ivp(est_instance.dynamics_deri, t_span, est_instance.get_states_init(),
                                   t_eval=t_explore,**est_instance.get_int_key(),args=(u_func_partial,)).y.T)
            if self.ExpObj == "A_Optimal":           
                costs[i] = self.A_optimal_cost(x_hist,u_hist,est_instance,ignore_frac=0)
            elif self.ExpObj == "E_Optimal":
                costs[i] =  self.E_optimal_cost(x_hist,u_hist,est_instance,ignore_frac=0) 
            elif self.ExpObj == "D_Optimal":
                costs[i] =  self.D_optimal_cost(x_hist,u_hist,est_instance,ignore_frac=0)          
        
        if self.ExpObj == "A_Optimal":
            # FW approximation for minimising tr(inv(cov))
            idx = torch.argmax(costs)
        elif self.ExpObj == "E_Optimal":
            # Maximising the minimum eigenvalue
            idx = torch.argmax(costs)
        elif self.ExpObj == "D_Optimal":
            # Minimising the log determinant of covariance
            idx = torch.argmin(costs)

        print("Frequency Chosen : ", frequencies[idx])

        return frequencies[idx]
    
    def A_optimal_cost(self,x_horizon,u_horizon,est_instance,ignore_frac=0):
        
        n_data = len(u_horizon)
        covs = torch.zeros_like(est_instance.get_cov(x_horizon[0],u_horizon[0]))
        for i in range(int(ignore_frac*n_data),n_data):
            covs = covs + est_instance.get_cov(x_horizon[i],u_horizon[i])
        
        R = self.hess_to_cost(covs,est_instance.dimx)
        cost = torch.trace(R.type(torch.FloatTensor) @ covs.type(torch.FloatTensor))
        return cost
    
    def hess_to_cost(self,cov,dimx):
        # compute quadratic cost to use at step n of online FW procedure
        dimz = cov.shape[0]
        R = torch.zeros(dimz,dimz)
        cov_inv = torch.linalg.inv(cov + 0.0001*torch.eye(cov.shape[0]))
        for i in range(dimz):
            for j in range(dimz):
                eij = torch.zeros(dimz,dimz)
                eij[i,j] = 1
                temp = torch.kron(torch.eye(dimx).type(torch.DoubleTensor), cov_inv.type(torch.DoubleTensor) @ eij.type(torch.DoubleTensor) @ cov_inv.type(torch.DoubleTensor))
                R[i,j] = torch.trace(temp)
        e,_ = torch.linalg.eig(R)

        if torch.min(torch.real(e)) < 0:
            R = R.T @ R
            U,S,V = torch.svd(R)
            R = U @ torch.diag(torch.sqrt(S)) @ U.T

        R = R / torch.max(R)
        R = R / torch.linalg.matrix_norm(R,2)
        return R.detach()
    
    def E_optimal_cost(self,x_horizon,u_horizon,est_instance,ignore_frac=0):
        n_data = len(u_horizon)
        covs = torch.zeros_like(est_instance.get_cov(x_horizon[0],u_horizon[0]))
        for i in range(int(ignore_frac*n_data),n_data):
            covs = covs + est_instance.get_cov(x_horizon[i],u_horizon[i])

        e,_ = torch.linalg.eig(covs)
        cost = torch.min(torch.real(e))
        return cost
    
    def D_optimal_cost(self,x_horizon,u_horizon,est_instance,ignore_frac=0):
        n_data = len(u_horizon)
        covs = torch.zeros_like(est_instance.get_cov(x_horizon[0],u_horizon[0]))
        for i in range(int(ignore_frac*n_data),n_data):
            covs = covs + est_instance.get_cov(x_horizon[i],u_horizon[i])

        cost = torch.log(torch.linalg.det(covs))
        return cost




# Old hyperparameter frequency_find
    def hyperparameters_find(self, est_instance, n_explore, n_data):

        hyperparams = self.random_parameters(n_explore)
        costs = torch.zeros(n_explore)
        
        # for i in range(n_explore):
        #     t_explore = torch.arange(0,self.H*self.sim_frac,est_instance.dt)
        #     t_span = (t_explore[0], t_explore[-1])

        #     u_func = self.forcing.control_function(hyperparams=hyperparams[i])
        #     u_func_partial = partial(u_func)
        #     u_hist = self.forcing.control_data(hyperparams[0],t_explore,est_instance.dimu)
            
        #     x_hist = torch.tensor(solve_ivp(est_instance.dynamics_deri, t_span, est_instance.get_states_init(),
        #                             t_eval=t_explore,**est_instance.get_int_key(),args=(u_func_partial,)).y.T)        
        #     if self.ExpObj == "A_Optimal":           
        #         costs[i] = self.optimality.A_optimal_cost(x_hist,u_hist,est_instance,ignore_frac=0)
        #     elif self.ExpObj == "E_Optimal":
        #         costs[i] =  self.optimality.E_optimal_cost(x_hist,u_hist,est_instance,ignore_frac=0) 
        #     elif self.ExpObj == "D_Optimal":
        #         costs[i] =  self.optimality.D_optimal_cost(x_hist,u_hist,est_instance,ignore_frac=0)

        est_instance_arg = est_instance.get_all_params()
        with ProcessPoolExecutor(max_workers=self.n_processes) as executor:
            # Submit tasks and gather futures
            futures = [executor.submit(self.cost_find, est_instance_arg, hyperparams[i], i) for i in range(n_explore)]

            # Retrieve results
            results = [future.result() for future in futures]

        # with Pool(processes=self.n_processes) as pool:
        #     results = pool.starmap(self.cost_find, [(est_instance, hyperparams[i], i) for i in range(n_explore)])

        # for idx, cost in results:
        #     costs[idx] = cost

        # Update costs based on the results
        for idx, cost in results:
            costs[idx] = cost

        if self.ExpObj == "A_Optimal":
            # FW approximation for minimising tr(inv(cov))
            idx = torch.argmax(costs)
        elif self.ExpObj == "E_Optimal":
            # Maximising the minimum eigenvalue
            idx = torch.argmax(costs)
        elif self.ExpObj == "D_Optimal":
            # Minimising the log determinant of covariance
            idx = torch.argmin(costs)
        else:
            raise ValueError("Unrecognized Optimality Criteria: {}".format(self.ExpObj))

        hyperparams_chosen = np.array([hyperparams[idx]])
        print("Hyperparameters Chosen : ", hyperparams_chosen)

        return hyperparams_chosen



        # if self.ExpObj == "A_Optimal":           
        #     cost = self.optimality.A_optimal_cost(x_hist,u_hist,est_instance_parallel,ignore_frac=0)
        # elif self.ExpObj == "E_Optimal":
        #     cost =  self.optimality.E_optimal_cost(x_hist,u_hist,est_instance_parallel,ignore_frac=0) 
        # elif self.ExpObj == "D_Optimal":
        #     cost =  self.optimality.D_optimal_cost(x_hist,u_hist,est_instance_parallel,ignore_frac=0)
        # else:
        #     raise ValueError("Unrecognized Optimality Criteria: {}".format(self.ExpObj))


# Debugging hyperparameter
import torch
import numpy as np
from functools import partial
from scipy.integrate import solve_ivp
from multiprocessing import Pool
from concurrent.futures import ProcessPoolExecutor, wait, ALL_COMPLETED
from Kernel import HIVKernel
from Environment import Environment
import multiprocessing
lock = multiprocessing.Lock()

class hyperparameters_find:
    
    def __init__(self,H,n_processes,sim_frac,signal_type,ExpObj):
        self.H = H
        self.n_processes = n_processes
        self.sim_frac = sim_frac
        self.signal_type = signal_type
        self.ExpObj = ExpObj

        self.integrator_keywords = {}
        self.integrator_keywords["rtol"] = 1e-8
        self.integrator_keywords["method"] = "LSODA"
        self.integrator_keywords["atol"] = 1e-8
        self.kernel = HIVKernel()

        self.forcing = ControlInput(self.signal_type)
        self.optimality = OptimalityCriteria(self.ExpObj)

    def random_parameters(self, n_explore):
        
        if self.signal_type=="SinWave":
            frequencies = np.random.uniform(0.5, 15, size=n_explore)
            hyperparams = frequencies.reshape(-1,1)
        elif self.signal_type=="Sin2Wave":
            frequencies1 = np.random.uniform(0.1, 15, size=n_explore)
            frequencies2 = np.random.uniform(0.01, 1.5, size=n_explore)
            hyperparams = np.column_stack((frequencies1, frequencies2))
        elif self.signal_type=="SchroederSweep":
            period = np.random.uniform(0.1,100, size=n_explore)
            n_harmonics = np.random.randint(1, 20+1, size=n_explore)
            hyperparams = np.column_stack((period, n_harmonics))
        elif self.signal_type=="ChirpLinear":
            f0 = np.random.uniform(0.1,10, size=n_explore)
            # f0 = np.ones(n_explore)
            t1 = np.random.uniform(0.1,10, size=n_explore)
            hyperparams = np.column_stack((f0, t1))
        else:
            raise ValueError("Unrecognized Signal Type: {}".format(self.signal_type))

        return hyperparams

    def hyperparameters_find(self, est_instance, n_explore, n_data):

        hyperparams = self.random_parameters(n_explore)
        costs = torch.zeros(n_explore)

        t_explore = torch.arange(0,self.H*self.sim_frac,est_instance.dt)
        t_span = (t_explore[0], t_explore[-1])
        time_arg = (t_explore, t_span)
        est_instance_arg = est_instance.get_all_params()
        with ProcessPoolExecutor(max_workers=self.n_processes) as executor:
            # Submit tasks and gather futures
            futures = [executor.submit(self.cost_find, est_instance_arg, time_arg, hyperparams[i], i) for i in range(n_explore)]
            completed_futures, _ = wait(futures, return_when=ALL_COMPLETED)
            # Retrieve results
            results = [future.result() for future in completed_futures]

        # Update costs based on the results
        for idx, cost in results:
            costs[idx] = cost

        if self.ExpObj == "A_Optimal":
            # FW approximation for minimising tr(inv(cov))
            idx = torch.argmax(costs)
        elif self.ExpObj == "E_Optimal":
            # Maximising the minimum eigenvalue
            idx = torch.argmax(costs)
        elif self.ExpObj == "D_Optimal":
            # Minimising the log determinant of covariance
            idx = torch.argmin(costs)
        else:
            raise ValueError("Unrecognized Optimality Criteria: {}".format(self.ExpObj))

        hyperparams_chosen = np.array([hyperparams[idx]])
        print("Hyperparameters Chosen : ", hyperparams_chosen)

        return hyperparams_chosen

    def cost_find(self, instance_arg, time_arg, hyperparams, idx):
        with lock:
            A,dimu,noise_cov,states_init,dt = instance_arg
            kernel = HIVKernel()
            est_instance_parallel = Environment(A=A,dimu=dimu,noise_cov=noise_cov,states_init=states_init,kernel=kernel,dt=dt)
            
            # self.forcing = ControlInput(self.signal_type)
            # optimality = OptimalityCriteria(self.ExpObj)

            t_explore, t_span = time_arg

            u_func = self.forcing.control_function(hyperparams=hyperparams)
            u_func_partial = partial(u_func)
            u_hist = self.forcing.control_data(hyperparams,t_explore,dimu)

            x_hist = torch.tensor(solve_ivp(est_instance_parallel.dynamics_deri, t_span, states_init,
                                    t_eval=t_explore,**self.integrator_keywords,args=(u_func_partial,)).y.T)
            cost = self.optimality.cost(x_hist,u_hist,est_instance_parallel,ignore_frac=0)  

            # n_data = len(u_hist)
            # phi = kernel.phi(x_hist[0],u_hist[0])
            # temp = torch.outer(phi.flatten(),phi.flatten())
            # covs = torch.zeros_like(temp)
            # for i in range(0,n_data):
            #     phi = kernel.phi(x_hist[i],u_hist[i])
            #     covs = covs + torch.outer(phi.flatten(),phi.flatten())

            # cost = torch.log(torch.linalg.det(covs))

        return idx, cost
    
class ControlInput:
    def __init__(self, signal_type):
        self.signal_type = signal_type

    def control_function(self, hyperparams):
        # One set of hyperparameters only (1D)
        if self.signal_type=="SinWave":
            def u_func(t):
                # sin(frequency*t)
                return np.column_stack([np.sin(hyperparams[0]*t)])
        elif self.signal_type=="Sin2Wave":
            def u_func(t):
                # sin(frequency1*t) + sin(frequency2*t)^2
                u = np.sin(hyperparams[0]*t) + np.sin(hyperparams[1]*t)**2
                return np.column_stack([u])
        elif self.signal_type=="SchroederSweep":
            Pf = hyperparams[0]
            K = int(hyperparams[1])
            def u_func(t):
                u = np.zeros_like(t)
                for i in range(1, K+1):
                    theta = 2 * np.pi / K * np.sum(np.arange(1, i+1))
                    u = u + np.array(np.sqrt(2/K) * np.cos(2 * np.pi * i * t / Pf + theta))
                return np.column_stack([u])
        elif self.signal_type=="ChirpLinear":
            # require debugging
            f0 = 0
            f1 = hyperparams[0]
            t1 = hyperparams[1]
            phi = 0
            beta = (f1 - f0) * (1 / t1)
            def u_func(t):
                u = np.cos(2 * np.pi * (beta / 2 * (t ** 2) + f0 * t + np.radians(phi) / 360))
                return np.column_stack([u])
        return u_func
    
    def control_data(self, hyperparams, t_explore, dimu):
        # One set of hyperparameters only (1D)
        if self.signal_type=="SinWave":
            u_hist = torch.sin(hyperparams[0]*t_explore).reshape(len(t_explore),dimu)
        elif self.signal_type=="Sin2Wave":
            u = torch.sin(hyperparams[0]*t_explore) + torch.sin(hyperparams[1]*t_explore)**2
            u_hist = u.reshape(len(t_explore),dimu)
        elif self.signal_type=="SchroederSweep":
            u_func = self.control_function(hyperparams=hyperparams)
            u_hist = u_func(t_explore)
        elif self.signal_type=="ChirpLinear":
            # f0 = 0
            f1 = hyperparams[0]
            t1 = hyperparams[1]
            # phi = 0
            u_hist = torch.cos(2 * np.pi * ((f1 / t1) / 2 * (t_explore ** 2))).reshape(len(t_explore),dimu)

        return u_hist       

class OptimalityCriteria:

    def __init__(self,optimality_criteria):
        self.optimality_criteria = optimality_criteria

    def cost(self,x_horizon,u_horizon,est_instance,ignore_frac=0):
        if self.optimality_criteria == "A_Optimal":           
            cost = self.A_optimal_cost(x_horizon,u_horizon,est_instance,ignore_frac=0)
        elif self.optimality_criteria == "E_Optimal":
            cost =  self.E_optimal_cost(x_horizon,u_horizon,est_instance,ignore_frac=0)
        elif self.optimality_criteria == "D_Optimal":
            cost =  self.D_optimal_cost(x_horizon,u_horizon,est_instance,ignore_frac=0)
        else:
            raise ValueError("Unrecognized Optimality Criteria: {}".format(self.optimality_criteria))
        return cost

    def A_optimal_cost(self,x_horizon,u_horizon,est_instance,ignore_frac=0):
        
        n_data = len(x_horizon)
        covs = torch.zeros_like(est_instance.get_cov(x_horizon[0],u_horizon[0]))
        for i in range(int(ignore_frac*n_data),n_data):
            covs = covs + est_instance.get_cov(x_horizon[i],u_horizon[i])
        
        R = self.hess_to_cost(covs,est_instance.dimx)
        cost = torch.trace(R.type(torch.FloatTensor) @ covs.type(torch.FloatTensor))
        return cost
    
    def hess_to_cost(self,cov,dimx):
        # compute quadratic cost to use at step n of online FW procedure
        dimz = cov.shape[0]
        R = torch.zeros(dimz,dimz)
        cov_inv = torch.linalg.inv(cov + 0.0001*torch.eye(cov.shape[0]))
        for i in range(dimz):
            for j in range(dimz):
                eij = torch.zeros(dimz,dimz)
                eij[i,j] = 1
                temp = torch.kron(torch.eye(dimx).type(torch.DoubleTensor), cov_inv.type(torch.DoubleTensor) @ eij.type(torch.DoubleTensor) @ cov_inv.type(torch.DoubleTensor))
                R[i,j] = torch.trace(temp)
        e,_ = torch.linalg.eig(R)

        if torch.min(torch.real(e)) < 0:
            R = R.T @ R
            U,S,V = torch.svd(R)
            R = U @ torch.diag(torch.sqrt(S)) @ U.T

        R = R / torch.max(R)
        R = R / torch.linalg.matrix_norm(R,2)
        return R.detach()
    
    def E_optimal_cost(self,x_horizon,u_horizon,est_instance,ignore_frac=0):
        n_data = len(x_horizon)
        covs = torch.zeros_like(est_instance.get_cov(x_horizon[0],u_horizon[0]))
        for i in range(int(ignore_frac*n_data),n_data):
            covs = covs + est_instance.get_cov(x_horizon[i],u_horizon[i])

        e,_ = torch.linalg.eig(covs)
        cost = torch.min(torch.real(e))
        return cost
    
    def D_optimal_cost(self,x_horizon,u_horizon,est_instance,ignore_frac=0):
        n_data = len(x_horizon)
        print("Number of data points to be considered : ", n_data)
        covs = torch.zeros_like(est_instance.get_cov(x_horizon[0],u_horizon[0]))
        for i in range(int(ignore_frac*n_data),n_data):
            covs = covs + est_instance.get_cov(x_horizon[i],u_horizon[i])

        cost = torch.log(torch.linalg.det(covs))
        return cost

# Old solve_ivp
# x_hist = torch.tensor(solve_ivp(est_instance_parallel.dynamics_deri, t_span, states_init,
#                         t_eval=t_explore,**self.integrator_keywords,args=(u_func_partial,)).y.T)


# Old metric
    def normalize(self,x):
        normalized_trajectory = (x - self.mean_true) / self.std_true
        return normalized_trajectory
    
    def mean_error(self,x_true,x_est):
        diff = (x_true - x_est)
        me = torch.mean(diff, dim=0)
        # rmse = torch.sqrt(mse)
        print(me)
        rmse_mean = torch.mean(me).item()
        return rmse_mean

        self.mean_true = torch.mean(self.x_true, dim=0, keepdim=True)
        self.std_true = torch.std(self.x_true, dim=0, keepdim=True)
        self.x_normalized_true = self.normalize(self.x_true)
